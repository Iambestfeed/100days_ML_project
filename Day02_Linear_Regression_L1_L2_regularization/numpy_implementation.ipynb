{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def MSE(y_pred : np.array, y : np.array):\n",
    "    return np.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class LinearRegressionImplement:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, l1_reg=0.0, l2_reg=0.0, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "\n",
    "        Parameters:\n",
    "            learning_rate (float): The learning rate for gradient descent.\n",
    "            epochs (int): Number of training iterations.\n",
    "            l1_reg (float): L1 regularization coefficient (lambda1).\n",
    "            l2_reg (float): L2 regularization coefficient (lambda2).\n",
    "            tol (float): Tolerance for early stopping. Training stops if the absolute\n",
    "                         improvement in loss between epochs is less than tol.\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate       # Learning rate for gradient descent\n",
    "        self.epochs = epochs          # Maximum number of training epochs\n",
    "        self.l1_reg = l1_reg          # L1 regularization parameter (λ₁)\n",
    "        self.l2_reg = l2_reg          # L2 regularization parameter (λ₂)\n",
    "        self.tol = tol                # Tolerance for early stopping based on loss improvement\n",
    "        self.weights = None           # Model parameters (weights + bias)\n",
    "        self.loss_history = []        # List to track the MSE loss at each epoch\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model to the training data using Stochastic Gradient Descent.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "            y (np.array): Target vector.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted model.\n",
    "        \"\"\"\n",
    "        # Add a bias term (column of ones) to the input feature matrix X\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        \n",
    "        # Initialize weights randomly (including bias)\n",
    "        n_features = X_b.shape[1]\n",
    "        self.weights = np.random.randn(n_features)\n",
    "        \n",
    "        # Training loop over epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle the data to avoid order bias\n",
    "            shuffled_indices = np.random.permutation(X_b.shape[0])\n",
    "            X_shuffled = X_b[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            \n",
    "            # Update weights for each data point (Stochastic Gradient Descent)\n",
    "            for i in range(X_shuffled.shape[0]):\n",
    "                xi = X_shuffled[i]\n",
    "                yi = y_shuffled[i]\n",
    "                \n",
    "                # Compute prediction for the current sample and the error\n",
    "                y_pred = np.dot(xi, self.weights)\n",
    "                error = y_pred - yi\n",
    "                \n",
    "                # Compute gradient from the MSE loss: (y_pred - y) * xi\n",
    "                grad = error * xi\n",
    "                \n",
    "                # Create a mask to avoid regularizing the bias term (first element)\n",
    "                reg_mask = np.concatenate(([0], np.ones(len(self.weights) - 1)))\n",
    "                \n",
    "                # Compute the gradient from L2 regularization: λ₂ * weight\n",
    "                grad_l2 = self.l2_reg * self.weights * reg_mask\n",
    "                \n",
    "                # Compute the subgradient from L1 regularization: λ₁ * sign(weight)\n",
    "                grad_l1 = self.l1_reg * np.sign(self.weights) * reg_mask\n",
    "                \n",
    "                # Total gradient is the sum of the loss gradient and regularization gradients\n",
    "                total_grad = grad + grad_l2 + grad_l1\n",
    "                \n",
    "                # Update the weights using the gradient descent rule\n",
    "                self.weights -= self.lr * total_grad\n",
    "            \n",
    "            # Calculate the loss over the entire dataset for monitoring after each epoch\n",
    "            y_pred_all = np.dot(X_b, self.weights)\n",
    "            mse = MSE(y_pred_all, y)\n",
    "            self.loss_history.append(mse)\n",
    "            \n",
    "            # Optionally print the loss every 10 epochs (or adjust as desired)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss {mse}\")\n",
    "            \n",
    "            # Early stopping: stop if the absolute improvement is below the tolerance\n",
    "            if epoch > 0:\n",
    "                loss_improvement = np.abs(self.loss_history[-2] - mse)\n",
    "                if loss_improvement < self.tol:\n",
    "                    print(f\"Early stopping at epoch {epoch}. Loss improvement {loss_improvement} is below tolerance {self.tol}.\")\n",
    "                    break\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained linear regression model.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Predicted values.\n",
    "        \"\"\"\n",
    "        # Add a bias term (column of ones) to the input feature matrix X\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        return np.dot(X_b, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "(n, m) = (100, 1)\n",
    "X = np.random.rand(n, m)  # Shape (100, 3)\n",
    "w = [2]\n",
    "bias = 3\n",
    "y = bias + X@w + np.random.randn(n)  # y = 3 + 2X1 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.8574816925743982\n",
      "Epoch 5: Loss 0.8613410625071986\n",
      "Epoch 10: Loss 0.8321739055416505\n",
      "Epoch 15: Loss 0.8179377750676542\n",
      "Epoch 20: Loss 0.8116299100442721\n",
      "Epoch 25: Loss 0.8096940268248987\n",
      "Early stopping at epoch 26. Loss improvement 9.444656018553488e-05 is below tolerance 0.0001.\n",
      "Weights: [bias, coefficients]: [3.18006317 1.68976879]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the model\n",
    "model = LinearRegressionImplement(learning_rate=0.01, epochs=50, l1_reg=0.001, l2_reg=0.002)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Final weights (bias term is the first element)\n",
    "print(\"Weights: [bias, coefficients]:\", model.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "(n, m) = (100, 3)\n",
    "X = 2 * np.random.rand(n, m)  # Shape (100, 3)\n",
    "w = [2, 1.5, -0.5]\n",
    "bias = 3\n",
    "y = bias + X@w + np.random.randn(n)  # y = 3 + 2X1 + 1.5X2 - 0.5X3 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 1.8970453822262159\n",
      "Epoch 5: Loss 1.0214239770044546\n",
      "Epoch 10: Loss 0.9498859630879611\n",
      "Epoch 15: Loss 0.9344863645894534\n",
      "Epoch 20: Loss 0.9287210939837265\n",
      "Epoch 25: Loss 0.945166321569708\n",
      "Epoch 30: Loss 0.9249811330352901\n",
      "Epoch 35: Loss 0.925795321017349\n",
      "Early stopping at epoch 38. Loss improvement 5.2096268449264294e-05 is below tolerance 0.0001.\n",
      "Weights: [bias, coefficients]: [ 2.67869561  2.13328726  1.43005693 -0.19545391]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the model\n",
    "model = LinearRegressionImplement(learning_rate=0.01, epochs=50, l1_reg=0.001, l2_reg=0.002)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Final weights (bias term is the first element)\n",
    "print(\"Weights: [bias, coefficients]:\", model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
