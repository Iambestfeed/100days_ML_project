{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute softmax activation.\n",
    "    \n",
    "    Parameters:\n",
    "        z (np.array): Linear combination input of shape (n_samples, n_classes)\n",
    "            \n",
    "    Returns:\n",
    "        np.array: Softmax probabilities\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    exp = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "class SoftmaxRegressionImplement:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, batch_size=32, tol=1e-3, patience=10):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.patience = patience\n",
    "        self.weights = None\n",
    "        self.loss_history = []\n",
    "        self.n_classes = None\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute categorical cross-entropy loss.\n",
    "\n",
    "        Parameters:\n",
    "            y (np.array): True labels (one-hot encoded)\n",
    "            y_pred (np.array): Predicted probabilities\n",
    "\n",
    "        Returns:\n",
    "            float: The categorical cross-entropy loss\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
    "\n",
    "    def to_one_hot(self, y):\n",
    "        \"\"\"Convert label vector to one-hot encoded matrix.\"\"\"\n",
    "        one_hot = np.zeros((y.shape[0], self.n_classes))\n",
    "        one_hot[np.arange(y.shape[0]), y] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"\n",
    "        Train the softmax regression model using Mini-batch SGD.\n",
    "        \n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix\n",
    "            y (np.array): Target vector (class labels)\n",
    "            verbose (bool): If True, print loss progress\n",
    "        \"\"\"\n",
    "        # Get number of classes\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = self.to_one_hot(y)\n",
    "        \n",
    "        # Add bias term\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        n_samples, n_features = X_b.shape\n",
    "\n",
    "        # Initialize weights for all classes\n",
    "        self.weights = np.random.randn(n_features, self.n_classes) * 0.01\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float(\"inf\")\n",
    "        no_improve_count = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_b[indices]\n",
    "            y_shuffled = y_one_hot[indices]\n",
    "\n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                z = np.dot(X_batch, self.weights)\n",
    "                y_pred = softmax(z)\n",
    "\n",
    "                # Compute gradient\n",
    "                error = y_pred - y_batch\n",
    "                grad = np.dot(X_batch.T, error) / len(y_batch)\n",
    "\n",
    "                # Update weights\n",
    "                self.weights -= self.lr * grad\n",
    "\n",
    "            # Compute loss over full dataset\n",
    "            z_all = np.dot(X_b, self.weights)\n",
    "            y_pred_all = softmax(z_all)\n",
    "            loss = self.compute_loss(y_one_hot, y_pred_all)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            if verbose and epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss < best_loss - self.tol:\n",
    "                best_loss = loss\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best loss: {best_loss:.6f}\")\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        z = np.dot(X_b, self.weights)\n",
    "        return softmax(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.820129\n",
      "Epoch 5: Loss = 0.501041\n",
      "Epoch 10: Loss = 0.424129\n",
      "Epoch 15: Loss = 0.383676\n",
      "Epoch 20: Loss = 0.356190\n",
      "Epoch 25: Loss = 0.335218\n",
      "Epoch 30: Loss = 0.317913\n",
      "Epoch 35: Loss = 0.302942\n",
      "Epoch 40: Loss = 0.289778\n",
      "Epoch 45: Loss = 0.278012\n",
      "Epoch 50: Loss = 0.267512\n",
      "Epoch 55: Loss = 0.258033\n",
      "Epoch 60: Loss = 0.249207\n",
      "Epoch 65: Loss = 0.241185\n",
      "Epoch 70: Loss = 0.233779\n",
      "Epoch 75: Loss = 0.226923\n",
      "Epoch 80: Loss = 0.220632\n",
      "Epoch 85: Loss = 0.214768\n",
      "Epoch 90: Loss = 0.209310\n",
      "Epoch 95: Loss = 0.204215\n",
      "\n",
      "Test accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "    \n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "    \n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "    \n",
    "# Train model\n",
    "model = SoftmaxRegressionImplement(\n",
    "    learning_rate=0.1, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    tol=1e-4, \n",
    "    patience=10\n",
    ")\n",
    "model.fit(X_train, y_train, verbose=True)\n",
    "    \n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
