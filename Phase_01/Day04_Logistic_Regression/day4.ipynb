{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid activation.\n",
    "        \n",
    "    Parameters:\n",
    "        z (np.array): Linear combination input.\n",
    "            \n",
    "    Returns:\n",
    "        np.array: Sigmoid activation of z.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegressionImplement:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, batch_size=32, tol=1e-3, patience=10):\n",
    "        \"\"\"\n",
    "        Initialize the Logistic Regression model using Mini-batch SGD.\n",
    "\n",
    "        Parameters:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            epochs (int): Number of training iterations.\n",
    "            batch_size (int): Number of samples per mini-batch.\n",
    "            tol (float): Tolerance for early stopping (minimum loss improvement).\n",
    "            patience (int): Number of epochs to wait before early stopping.\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.patience = patience\n",
    "        self.weights = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "\n",
    "        Parameters:\n",
    "            y (np.array): True binary labels.\n",
    "            y_pred (np.array): Predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "            float: The BCE loss.\n",
    "        \"\"\"\n",
    "        # Add epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model using Mini-batch Stochastic Gradient Descent.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "            y (np.array): Target vector.\n",
    "            verbose (bool): If True, print loss progress.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted model.\n",
    "        \"\"\"\n",
    "        # Add bias term (a column of ones)\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        n_samples, n_features = X_b.shape\n",
    "\n",
    "        # Initialize weights randomly\n",
    "        self.weights = np.random.randn(n_features)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float(\"inf\")\n",
    "        no_improve_count = 0\n",
    "\n",
    "        # Training loop over epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data at the start of each epoch\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_b[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                # Compute linear predictions and then probabilities using sigmoid\n",
    "                z = np.dot(X_batch, self.weights)\n",
    "                y_pred = sigmoid(z)\n",
    "\n",
    "                # Compute the error (predicted probabilities minus true labels)\n",
    "                error = y_pred - y_batch\n",
    "\n",
    "                # Compute gradient\n",
    "                grad = np.dot(X_batch.T, error) / len(y_batch)\n",
    "\n",
    "                # Update weights with SGD\n",
    "                self.weights -= self.lr * grad\n",
    "\n",
    "            # Compute loss over the full dataset for monitoring\n",
    "            z_all = np.dot(X_b, self.weights)\n",
    "            y_pred_all = sigmoid(z_all)\n",
    "            loss = self.compute_loss(y, y_pred_all)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            if verbose and epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss < best_loss - self.tol:\n",
    "                best_loss = loss\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best loss: {best_loss:.6f}\")\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities for the input features.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Predicted probabilities.\n",
    "        \"\"\"\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        z = np.dot(X_b, self.weights)\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict binary labels for the input features.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Binary predictions (0 or 1).\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.557525\n",
      "Epoch 5: Loss = 0.378728\n",
      "Epoch 10: Loss = 0.352244\n",
      "Epoch 15: Loss = 0.350487\n",
      "Epoch 20: Loss = 0.350374\n",
      "Epoch 25: Loss = 0.350365\n",
      "Early stopping at epoch 27. Best loss: 0.350406\n",
      "Test Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegressionImplement(learning_rate=0.1, epochs=100, batch_size=32, tol=1e-4, patience=10)\n",
    "model.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate and print the test accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
