{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "\n",
    "np.random.seed(1022025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def MSE(y_pred : np.array, y : np.array):\n",
    "    return np.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class LinearRegressionImplement:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, batch_size=32, l1_reg=0.0, l2_reg=0.0, tol=1e-3, patience=10):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model using Mini-batch SGD.\n",
    "\n",
    "        Parameters:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            epochs (int): Number of training iterations.\n",
    "            batch_size (int): Number of samples per mini-batch.\n",
    "            l1_reg (float): L1 regularization coefficient (λ1).\n",
    "            l2_reg (float): L2 regularization coefficient (λ2).\n",
    "            tol (float): Tolerance for early stopping (min loss improvement).\n",
    "            patience (int): Number of epochs to wait before early stopping.\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.tol = tol\n",
    "        self.patience = patience\n",
    "        self.weights = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y, verbose = False):\n",
    "        \"\"\"\n",
    "        Train the model using Mini-batch Stochastic Gradient Descent (Mini-batch SGD).\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "            y (np.array): Target vector.\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted model.\n",
    "        \"\"\"\n",
    "        # Add a bias term (column of ones)\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "        # Initialize weights randomly\n",
    "        n_samples, n_features = X_b.shape\n",
    "        self.weights = np.random.randn(n_features)\n",
    "\n",
    "        # Early stopping variables\n",
    "        best_loss = float(\"inf\")\n",
    "        no_improve_count = 0\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data\n",
    "            shuffled_indices = np.random.permutation(n_samples)\n",
    "            X_shuffled, y_shuffled = X_b[shuffled_indices], y[shuffled_indices]\n",
    "\n",
    "            # Process mini-batches\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i + self.batch_size]\n",
    "                y_batch = y_shuffled[i:i + self.batch_size]\n",
    "\n",
    "                # Compute predictions\n",
    "                y_pred = np.dot(X_batch, self.weights)\n",
    "                error = y_pred - y_batch\n",
    "\n",
    "                # Compute gradient\n",
    "                grad = np.dot(X_batch.T, error) / len(y_batch)\n",
    "\n",
    "                # Regularization terms (avoid bias regularization)\n",
    "                reg_mask = np.concatenate(([0], np.ones(n_features - 1)))\n",
    "                grad_l1 = self.l1_reg * np.sign(self.weights) * reg_mask\n",
    "                grad_l2 = self.l2_reg * self.weights * reg_mask\n",
    "\n",
    "                # Total gradient update\n",
    "                self.weights -= self.lr * (grad + grad_l1 + grad_l2)\n",
    "\n",
    "            # Compute loss after full epoch\n",
    "            y_pred_all = np.dot(X_b, self.weights)\n",
    "            mse = np.mean((y - y_pred_all) ** 2)\n",
    "            self.loss_history.append(mse)\n",
    "\n",
    "            # Print loss every 10 epochs\n",
    "            if epoch % 5 == 0 and verbose:\n",
    "                print(f\"Epoch {epoch}: Loss = {mse:.6f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if mse < best_loss - self.tol:\n",
    "                best_loss = mse\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= self.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}. Best loss: {best_loss:.6f}\")\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained linear regression model.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.array): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Predicted values.\n",
    "        \"\"\"\n",
    "        X_b = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        return np.dot(X_b, self.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 85. Best loss: 0.061318\n",
      "\n",
      "Evaluation Metrics Comparison:\n",
      "=================================================================\n",
      "Metric              Custom Model        Sklearn Model\n",
      "=================================================================\n",
      "MSE                 0.063270            0.064109\n",
      "MAE                 0.202562            0.196904\n",
      "R² Score            0.730673            0.727102\n",
      "Accuracy            0.956140            0.947368\n",
      "Training Time (s)   0.024916            0.007611\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  # Features and labels\n",
    "\n",
    "# Split dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# === Train Custom Model ===\n",
    "start_time = time.time()\n",
    "custom_model = LinearRegressionImplement(learning_rate=0.1, epochs=100, batch_size=64, l1_reg=0.001, l2_reg=0.005)\n",
    "custom_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "custom_time = end_time - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_custom = custom_model.predict(X_test)\n",
    "\n",
    "# === Train Sklearn Model ===\n",
    "start_time = time.time()\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "sklearn_time = end_time - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_sklearn = sklearn_model.predict(X_test)\n",
    "\n",
    "# === Evaluation Metrics ===\n",
    "def evaluate(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, np.round(y_pred))  # Round predictions to 0 or 1\n",
    "    return mse, mae, r2, accuracy\n",
    "\n",
    "mse_custom, mae_custom, r2_custom, acc_custom = evaluate(y_test, y_pred_custom)\n",
    "mse_sklearn, mae_sklearn, r2_sklearn, acc_sklearn = evaluate(y_test, y_pred_sklearn)\n",
    "\n",
    "# === Print Evaluation Table ===\n",
    "print(\"\\nEvaluation Metrics Comparison:\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Metric':<20}{'Custom Model':<20}{'Sklearn Model'}\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'MSE':<20}{mse_custom:<20.6f}{mse_sklearn:.6f}\")\n",
    "print(f\"{'MAE':<20}{mae_custom:<20.6f}{mae_sklearn:.6f}\")\n",
    "print(f\"{'R² Score':<20}{r2_custom:<20.6f}{r2_sklearn:.6f}\")\n",
    "print(f\"{'Accuracy':<20}{acc_custom:<20.6f}{acc_sklearn:.6f}\")\n",
    "print(f\"{'Training Time (s)':<20}{custom_time:<20.6f}{sklearn_time:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
